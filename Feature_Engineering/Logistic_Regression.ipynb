{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff726d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, random_state=42)\n",
    "y = y.reshape(-1, 1)  # Make it column vector\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cce478",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    return sigmoid(np.dot(X, weights))\n",
    "\n",
    "\n",
    "def compute_loss(y, y_pred):\n",
    "    epsilon = 1e-10  # prevent log(0)\n",
    "    return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
    "\n",
    "\n",
    "def train(X, y, lr=0.1, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    X = np.c_[np.ones((m, 1)), X]  # Add bias term\n",
    "    weights = np.zeros((n + 1, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(X, weights)\n",
    "        gradient = (1 / m) * np.dot(X.T, (y_pred - y))\n",
    "        weights -= lr * gradient\n",
    "        if epoch % 100 == 0:\n",
    "            loss = compute_loss(y, y_pred)\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "    return weights\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
