{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5a4ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Import necessary libraries ---\n",
    "# TensorFlow is the core library for building and training neural networks.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99761e32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# numpy is used for numerical operations, especially with arrays.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib is used to visualize the image for our prediction.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- TensorFlow and Libraries Loaded ---\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead46153",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Load the MNIST dataset ---\n",
    "# The dataset is conveniently available within Keras and is split into\n",
    "# training and testing sets automatically.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"\\n--- Dataset Loaded ---\")\n",
    "print(f\"Training data shape (images): {x_train.shape}\")\n",
    "print(f\"Training data shape (labels): {y_train.shape}\")\n",
    "print(f\"Testing data shape (images):  {x_test.shape}\")\n",
    "print(f\"Testing data shape (labels):  {y_test.shape}\")\n",
    "print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b93c69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Preprocess the data ---\n",
    "# Deep learning models work best with normalized data.\n",
    "# We'll normalize the pixel values from the range [0, 255] to [0, 1].\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231476c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CNNs expect a 4-dimensional input: (samples, height, width, channels).\n",
    "# Our images are currently 3D: (samples, height, width).\n",
    "# We'll add a channel dimension for grayscale (1 channel).\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f4097",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The labels (y_train, y_test) are single integers (e.g., 5).\n",
    "# We need to one-hot encode them for multi-class classification.\n",
    "# For example, the digit 5 becomes a vector [0, 0, 0, 0, 0, 1, 0, 0, 0, 0].\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"\\n--- Data Preprocessing Complete ---\")\n",
    "print(f\"Reshaped training images shape: {x_train.shape}\")\n",
    "print(f\"One-hot encoded training labels shape: {y_train.shape}\")\n",
    "print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f359c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. Build the Convolutional Neural Network (CNN) model ---\n",
    "# A sequential model is a linear stack of layers.\n",
    "model = tf.keras.models.Sequential([\n",
    "    # First convolutional layer with 32 filters, a 3x3 kernel,\n",
    "    # and ReLU activation. The input shape is specified for the first layer.\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    # Max pooling layer to downsample the feature maps.\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional layer with 64 filters.\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    # Another max pooling layer.\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten the 2D feature maps to a 1D vector to feed into the dense layers.\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Dense hidden layer with 128 units and ReLU activation.\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # Dropout layer to help prevent overfitting.\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Output layer with 10 units (for digits 0-9) and softmax activation.\n",
    "    # Softmax ensures the outputs sum to 1, representing probabilities.\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb3395",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Architecture Defined ---\")\n",
    "model.summary()\n",
    "print(\"-\" * 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cb156",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. Compile the model ---\n",
    "# We define the optimizer, loss function, and metrics for training.\n",
    "# Adam is a popular and effective optimizer.\n",
    "# Categorical cross-entropy is the standard loss for multi-class classification.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- 6. Train the model ---\n",
    "# We fit the model to our training data. We use 5 epochs to keep the runtime short.\n",
    "# An epoch is one full pass through the entire training dataset.\n",
    "print(\"\\n--- Training the Model (5 Epochs) ---\")\n",
    "history = model.fit(x_train, y_train, epochs=5, validation_split=0.1, verbose=1)\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983ea77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. Evaluate the model on the test data ---\n",
    "# We use the separate test set to evaluate how well the model generalizes to new data.\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee18bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 8. Make a prediction on a single image ---\n",
    "# Let's pick a random image from the test set to see what the model predicts.\n",
    "# We get the true label for comparison.\n",
    "index = np.random.randint(0, len(x_test))\n",
    "sample_image = x_test[index]\n",
    "true_label = np.argmax(y_test[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ffe18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The model expects a batch of images, so we need to add a batch dimension.\n",
    "# The input shape will be (1, 28, 28, 1).\n",
    "sample_image_batch = np.expand_dims(sample_image, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ebab60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction. The output is a probability distribution.\n",
    "predictions = model.predict(sample_image_batch)\n",
    "predicted_label = np.argmax(predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
