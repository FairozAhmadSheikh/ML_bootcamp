{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # Adjust paths as necessary\n",
    "    train_df = pd.read_csv('train.csv', parse_dates=['date'])\n",
    "    stores_df = pd.read_csv('stores.csv')\n",
    "    holidays_df = pd.read_csv('holidays_events.csv', parse_dates=['date'])\n",
    "    oil_df = pd.read_csv('oil.csv', parse_dates=['date'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files. Ensure all CSVs are in the correct directory. Details: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Merging \n",
    "print(\"Cleaning and merging data...\")\n",
    "\n",
    "# Standardize 'type' column name in holidays_df to avoid conflict\n",
    "holidays_df.rename(columns={'type': 'holiday_type'}, inplace=True)\n",
    "\n",
    "# Merge stores information with training data\n",
    "df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "\n",
    "# Merge holidays information\n",
    "# Note: Some dates have multiple holidays; the merge handles this.\n",
    "df = df.merge(holidays_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge oil price information\n",
    "# Backfill missing oil prices (often done in time series when price is constant)\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].fillna(method='ffill')\n",
    "df = df.merge(oil_df, on='date', how='left')\n",
    "\n",
    "# Fill NaNs created by the merge (e.g., if a day wasn't a holiday)\n",
    "df['holiday_type'] = df['holiday_type'].fillna('None')\n",
    "df['locale'] = df['locale'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Engineering: Time-Based Features \n",
    "print(\"Creating time-based features...\")\n",
    "\n",
    "# Sort the data by store and date (CRITICAL for time series)\n",
    "df = df.sort_values(by=['store_nbr', 'date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Core Temporal Features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['dayofweek'] = df['date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df['dayofyear'] = df['date'].dt.dayofyear\n",
    "df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "df['weekend'] = (df['date'].dt.dayofweek >= 5).astype(int) # 1 if Saturday/Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering: Lags (For ML Models) \n",
    "# Lags help the model remember previous sales, usually on a store-item level.\n",
    "print(\"Creating lag features...\")\n",
    "\n",
    "# Create 7-day lag for 'sales'\n",
    "# We use a shift() grouped by 'store_nbr' and 'family' to ensure we only look at\n",
    "# the history of that specific series.\n",
    "df['sales_lag_7'] = df.groupby(['store_nbr', 'family'])['sales'].shift(7)\n",
    "\n",
    "# Create 30-day rolling mean of 'sales'\n",
    "df['sales_rolling_mean_30'] = df.groupby(['store_nbr', 'family'])['sales'].transform(\n",
    "    lambda x: x.shift(7).rolling(30).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd45308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5. Feature Engineering: Categorical Encoding (For ML Models) \n",
    "print(\"Encoding categorical features...\")\n",
    "\n",
    "# Convert categorical columns to numerical using pd.get_dummies\n",
    "categorical_cols = ['store_nbr', 'family', 'store_type', 'cluster', 'holiday_type', 'locale']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# --- 6. Final Data Preparation ---\n",
    "# Drop the original 'date' column as we have extracted features from it\n",
    "df.drop(['date', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Final Data Preparation \n",
    "# Drop the original 'date' column as we have extracted features from it\n",
    "df.drop(['date', 'id'], axis=1, inplace=True) \n",
    "\n",
    "# Handle NaNs from the lag/rolling features by dropping or imputing\n",
    "# Dropping is common for the start of the time series where lags are undefined.\n",
    "df.dropna(inplace=True) \n",
    "\n",
    "print(\"\\n✅ Data Preparation Complete.\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define Features and Target (XGBoost)\n",
    "X_xgb = df.drop('sales', axis=1)\n",
    "y_xgb = df['sales']\n",
    "print(f\"XGBoost Feature Count: {X_xgb.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Time-Based Train/Test Split (XGBoost) \n",
    "# Use the first 90% of data for training, and the last 10% for testing.\n",
    "split_point = int(len(X_xgb) * 0.9)\n",
    "X_train_xgb, X_test_xgb = X_xgb.iloc[:split_point], X_xgb.iloc[split_point:]\n",
    "y_train_xgb, y_test_xgb = y_xgb.iloc[:split_point], y_xgb.iloc[split_point:]\n",
    "\n",
    "print(f\"XGBoost Train/Test Split: {X_train_xgb.shape} / {X_test_xgb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9205c",
   "metadata": {},
   "source": [
    "ML MODEL (XGBOOST)\n",
    "## PART 1: Machine Learning Implementation (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc725e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"\\n Starting XGBoost Training \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Train Model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', \n",
    "    n_estimators=500, \n",
    "    learning_rate=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d75434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_xgb, y_pred_xgb))\n",
    "print(f\"✅ XGBoost RMSE: {rmse_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d93568",
   "metadata": {},
   "source": [
    "### PART 2: Deep Learning Implementation (LSTM & CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c13f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Starting Deep Learning Preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load or reset data to avoid using one-hot encoded columns (which are bad for sequencing)\n",
    "# A clean start is necessary for the sequencing strategy.\n",
    "# We will use the 'df_pre_encode' from the earlier steps (assumed to be a version before one-hot encoding)\n",
    "# Re-Do Clean Merge (without extensive get_dummies) \n",
    "train_df = pd.read_csv('train.csv', parse_dates=['date'])\n",
    "stores_df = pd.read_csv('stores.csv')\n",
    "holidays_df = pd.read_csv('holidays_events.csv', parse_dates=['date'])\n",
    "oil_df = pd.read_csv('oil.csv', parse_dates=['date'])\n",
    "holidays_df.rename(columns={'type': 'holiday_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93442eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a master DataFrame for DL\n",
    "df_dl = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "df_dl = df_dl.merge(holidays_df, on='date', how='left')\n",
    "oil_df['dcoilwtico'] = oil_df['dcoilwtico'].fillna(method='ffill')\n",
    "df_dl = df_dl.merge(oil_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463de01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all non-numeric features that we won't manually engineer or encode\n",
    "df_dl.drop(['id', 'family', 'store_type', 'cluster', 'holiday_type', 'locale', 'locale_name', 'description'], axis=1, inplace=True)\n",
    "df_dl = df_dl.sort_values(by=['store_nbr', 'date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb121f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs in oil and mark non-business days\n",
    "df_dl['dcoilwtico'] = df_dl['dcoilwtico'].fillna(method='bfill') # Impute remaining NaNs\n",
    "\n",
    "#  Feature Scaling (CRITICAL for DL) \n",
    "scaler = MinMaxScaler()\n",
    "# Scale the target variable 'sales'\n",
    "df_dl['sales_scaled'] = scaler.fit_transform(df_dl[['sales']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Creation (Windowing)\n",
    "# Define sequence parameters\n",
    "SEQUENCE_LENGTH = 30 # Use the past 30 days of data\n",
    "FORECAST_HORIZON = 1 # Predict 1 day into the future\n",
    "\n",
    "def create_sequences(data, sequence_length, target_col):\n",
    "    \"\"\"Transforms a single time series column into input/output sequences.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length - FORECAST_HORIZON + 1):\n",
    "        # Input sequence: data from t to t + SEQUENCE_LENGTH - 1\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "        # Output target: data at t + SEQUENCE_LENGTH\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will focus on one store for simplicity in this example\n",
    "df_single_store = df_dl[df_dl['store_nbr'] == 1].copy()\n",
    "\n",
    "# Create sequences for the scaled sales target\n",
    "X_seq, y_seq = create_sequences(df_single_store['sales_scaled'].values, SEQUENCE_LENGTH, 'sales_scaled')\n",
    "\n",
    "# Split sequences into train/test sets\n",
    "split_seq_point = int(len(X_seq) * 0.9)\n",
    "X_train_seq, X_test_seq = X_seq[:split_seq_point], X_seq[split_seq_point:]\n",
    "y_train_seq, y_test_seq = y_seq[:split_seq_point], y_seq[split_seq_point:]\n",
    "\n",
    "# Reshape input for LSTM/CNN: [samples, timesteps, features]\n",
    "# Since we only used one feature ('sales_scaled'), we need to add a dimension.\n",
    "X_train_seq = X_train_seq.reshape(X_train_seq.shape[0], X_train_seq.shape[1], 1)\n",
    "X_test_seq = X_test_seq.reshape(X_test_seq.shape[0], X_test_seq.shape[1], 1)\n",
    "\n",
    "print(f\"DL Train/Test Split (Sequence Shape): {X_train_seq.shape} / {X_test_seq.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
